{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First execute the code cells below, and then start reading the module below. Be sure to start with executing the second cell will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the data file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('thresholded.zip', 'r')\n",
    "zip_ref.extractall('thresholded')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block will take a fair amount of time to run, since it is loading a large dataset of images\n",
    "# be sure that the thresholded.zip is unzipped.\n",
    "def get_biggest_contour(image):\n",
    "    ignore,contours,ignore = cv2.findContours(image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    areas = [cv2.contourArea(c) for c in contours]\n",
    "    index = np.argmax(areas)\n",
    "    return contours[index][:,0,:]\n",
    "    \n",
    "def load_contours():\n",
    "    files = os.listdir(\"thresholded\")\n",
    "    #files=files[:4000]\n",
    "    files.sort()\n",
    "    labels = []\n",
    "    contours = []\n",
    "    dataset_shapes = []\n",
    "    for n, f in enumerate(files):\n",
    "        img = plt.imread(\"thresholded/\" + f) > 0.5\n",
    "        dataset_shapes.append(img.shape)\n",
    "        contours.append(get_biggest_contour(img))\n",
    "        labels.append(f[:f.rfind(\"_\")])\n",
    "        if (n % 100 == 0):\n",
    "            print(\"%d%% \" % np.round(n/len(files) * 100), end = \"\")\n",
    "    \n",
    "    class_names = list(set(labels))\n",
    "    class_names.sort()\n",
    "    \n",
    "    class_dict = {}\n",
    "    rev_class_dict = {}\n",
    "    for n, c in enumerate(class_names):\n",
    "        class_dict[n] = c\n",
    "        rev_class_dict[c] = n\n",
    "    \n",
    "    num_labels = [rev_class_dict[c] for c in labels]\n",
    "        \n",
    "    return contours, num_labels, class_dict, dataset_shapes\n",
    "\n",
    "dataset_contours, dataset_labels, class_dict, dataset_shapes = load_contours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_contour(contour, image_shape):\n",
    "    # cv2.fillPoly(img, pts, color[, lineType[, shift[, offset]]]) \n",
    "    img = np.zeros(image_shape)\n",
    "    cv2.fillPoly(img, [contour], (1.0,1.0,1.0))\n",
    "    return img\n",
    "\n",
    "class ContourImageAccessor():\n",
    "    def __init__(self, contours, shapes):\n",
    "        self.contours = contours\n",
    "        self.shapes = shapes\n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        if (type(n) == int):\n",
    "            return get_image_from_contour(self.contours[n], self.shapes[n])\n",
    "        elif (type(n) == slice):\n",
    "            raise RuntimeError(\"Slicing not implemented\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown type passed to getitem\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(dataset_contours)\n",
    "    \n",
    "dataset_images = ContourImageAccessor(dataset_contours, dataset_shapes)\n",
    "\n",
    "plt.imshow(dataset_images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Shape Features\n",
    "\n",
    "In the previous module, we introduced basic concepts in machine learning/vision such as features, classifiers, decision boundaries, and training/test sets. In this module, we will have a closer look at features, particularly what makes a good choice of features.\n",
    "\n",
    "So far, we have only worked with some very basic features, area and color (specifically redness). To be successful in machine vision work, you need a large toolkit of feature choices. For a given problem, one kind of feature may be more appropriate than another. So, variety in options gives you a good starting point.\n",
    "\n",
    "In this module, we will introduce so-called \"contour features\". These are features that work on the outlines surrounding objects. To illustrate the concepts, we will be making use of a database of real leaf images called the Middle European Woods database (MEW 2012, 2014) (P. Novotný and T. Suk, Leaf recognition of woody species in Central Europe, Biosystems Engineering, vol. 115, no. 4, pp. 444–452, 2013).\n",
    "\n",
    "The MEW database does include color photos of leaves, but we are going to focus on the outlines of leaves in this module. Let's plot 5 outline images from the database to give you a feeling for how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_nums = [0, 1000, 2000, 3000, 4000]\n",
    "plt.rcParams[\"figure.figsize\"] = (4 * len(demo_nums),5)\n",
    "for n, dem in enumerate(demo_nums):\n",
    "    plt.subplot(1,len(demo_nums),n+1)\n",
    "    plt.imshow(dataset_images[dem])\n",
    "    plt.title(class_dict[dataset_labels[dem]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our task is to identify the species of a given leaf. Although the above examples look quite different from each other, you will find that many of the 153 species in the database have leaves which are difficult to distinguish, so this is a challenging problem. In some cases, outline features may not be enough to separate classes. However, the analysis will demonstrate that we can already get reasonably far just by looking at the object outlines. We can always increase the power of our model later by added other types of features.\n",
    "\n",
    "Up until now, we haven't discussed what make good image features. In the toy apple ripeness problem in the last module, we chose area and redness because these qualities are directly important in deciding ripeness (in the toy problem at least). For outlines, our choice isn't that clearcut. Certainly we could come up with qualities like \"roughness/smoothness\" or \"long and thin\", but how do we quantify this mathematically?\n",
    "\n",
    "In this module, we will look at two well-known kinds of contour features:\n",
    "* Hu moments\n",
    "* Fourier descriptors\n",
    "\n",
    "These features have demonstrated their usefulness in a large variety of outline recognition problems, so it is good to add them to your toolbox.\n",
    "\n",
    "More importantly, we will discuss the thinking behind these features, and why researchers came up with them in the first place. This is the kind of thought process you need to go through when selecting features for a new problem.\n",
    "\n",
    "Before continuing, let's list some qualities that are desirable in a set of features:\n",
    "* Objects from different classes should produce features that are distinct from each other. This point is non-negotiable, if there aren't sufficient differences in features from different classes, there is nothing on which a classifier can base its decisions. As an example, the toy ripe apples all have large areas, which contrasts with the small area of unripe apples. \n",
    "* Objects from the same class should produce similar features. For example, ripe apples all have large areas more or less within the same range. If features from a single class are spread over very different regions of the features space, it can be difficult for a classifier to learn a decision boundary that includes all these separate regions. Note this point is not essential, but whether or not the task will succeed would otherwise rely a lot on how flexible the classifier is.\n",
    "* Features should be robust, meaning that minor changes to a particular object shouldn't result in big changes to the features extracted from that object. For example, if identifying a leaf species, minor damage to the leaf shouldn't cause the extracted features to be dramatically different from an undamaged leaf. However, if your task is to detect damage, you *do* want features to reflect this.\n",
    "* Furthermore, the features from an object shouldn't be affected by (more technically, should be invariant with respect to) transformations like (for example) movement, rotation, scaling, mirroring and differing lighting conditions IF those changes are considered irrelevant to the classes in question. One basic example would be that a car is still a car if it is towards the left side or the right side of a given image. If your task is to classify whether or not a car is in the image, then this change shouldn't matter. A mirror image of a person should also still be recognised as a person. However, the mirror image of the letter \"b\" looks a lot like a \"d\", so in this case the features shouldn't be invariant to mirroring. It *is* possible to train a model in a way that includes these kinds of transformations in the training set, but this usually wastes model capacity in having to learn that these transformations don't matter. That places a lot of extra responsibility on the classifier itself, which may lead to failure.\n",
    "\n",
    "Finally, just keep in mind that, to the classifier, features are really just a bunch of numbers. Our task is to choose numbers that help the classifier perform its task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments-based Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our discussion of contour features with so-called \"moment\" features. Firstly, we motivate why moment features are useful by looking at the properties of some moments you know already, as well as some that you might not know yet. We present the mathematical background, but you should really just get a sense for what kinds of information are reflected in moments. In the end, we will show you how specifically Hu moments respond to changes like translation, rotation and scaling, and that it the most important thing to keep in mind. \n",
    "\n",
    "You are actually already familiar with some moment features. For instance the area are special cases of moment calculations. Also the x/-centroid, of which you can think about as the 2-dimensional version of the center of mass, is a moment. Let's have a careful look.\n",
    "\n",
    "Let your image be a rectangular $dim_x \\times dim_y$ array $I(x,y)$. So $x$ runs from 0 to $dim_x$ and and $y$ from 0 to $dim_y$. The general mathematical definition of moments in two dimensions is given by\n",
    "\n",
    "$$m_{kl} = \\sum_{x=0}^{dim_x}\\sum_{y=0}^{dim_y}I(x,y)x^ky^l.$$\n",
    "\n",
    "Here and $k$ and $l$ are integers greater or equal to 0. To have a cleaner notation we leave out the range of $x$ and $y$ form now on. Let's start with the special case $k=l=0$.\n",
    "\n",
    "$$m_{00} = \\sum_{x}\\sum_{y}I(x,y)x^0y^0 = \\sum_{x}\\sum_{y}I(x,y)$$\n",
    "\n",
    "Let's assume we are dealing with a binary image, pixels are either black ($I=0$), or white ($I=1$).This expression just sums each pixel in the image together. Since black pixels have a value of 0, and white pixels have a value of 1, this means we end up counting the number of white pixels. Therefore, $m_{00}$ is the area.\n",
    "\n",
    "Now look at the cases:\n",
    "\n",
    "$$m_{10} = \\sum_{x}\\sum_{y}I(x,y)x^1y^0 = \\sum_{x}\\sum_{y}I(x,y)x$$\n",
    "$$m_{01} = \\sum_{x}\\sum_{y}I(x,y)x^0y^1 = \\sum_{x}\\sum_{y}I(x,y)y$$\n",
    "\n",
    "If we were to divide these by $m_{00}$, we get the formulae for the x and y centroids (the average x value and average y value of all white pixels taken together):\n",
    "\n",
    "$$\\frac{m_{10}}{m_{00}} = \\frac{\\sum_{x}\\sum_{y}I(x,y)x}{\\sum_{x}\\sum_{y}I(x,y)}=\\bar{x}$$\n",
    "$$\\frac{m_{01}}{m_{00}} = \\frac{\\sum_{x}\\sum_{y}I(x,y)y}{\\sum_{x}\\sum_{y}I(x,y)}=\\bar{y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can calculate moments easily with OpenCV. Let's stop and visualize just the areas and centroids of some of the input images. The below code block will allow you to select an image from the database in which we display a red point at the centroid $(\\bar{x},\\bar{y})$ of the leaf region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_first_moments(n=(0,len(dataset_images)-1)):\n",
    "    plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "    moments = cv2.moments(dataset_images[n].astype(np.uint8))\n",
    "    plt.imshow(dataset_images[n], cmap=\"gray\")\n",
    "    area_fraction = moments[\"m00\"] / (dataset_images[n].shape[0] * dataset_images[n].shape[1])\n",
    "    area = moments[\"m00\"]\n",
    "    plt.suptitle(\"Leaf area %d%% of image\" % (area_fraction*100))\n",
    "    plt.title(\"Total area of leaf \" + str(int(moments[\"m00\"]))) \n",
    "    x_bar = moments[\"m10\"] / moments[\"m00\"]\n",
    "    y_bar = moments[\"m01\"] / moments[\"m00\"]\n",
    "    plt.scatter([x_bar], [y_bar], marker=\"o\", c=\"r\", s=50)\n",
    "\n",
    "interact(interact_first_moments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__\n",
    "For each of the image both the total area of the leaf and the fraction of the total area is given. What do you think is a better feature to use for a classifyer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you've got an intuitive feel for at least three of the moments, are there other interesting cases? There are also the so-called central moments, which are calculated relative to the centroid (as visualized above). They are exactly the same, except for the substraction of the centroid coordinates from x and y within the summation.\n",
    "\n",
    "$$\\mu_{kl} = \\sum_{x}\\sum_{y}I(x,y)(x-\\bar{x})^k(y-\\bar{y})^l$$\n",
    "\n",
    "Two of the central moments have a special meaning\n",
    "\n",
    "$$\\mu_{20} = \\sum_{x}\\sum_{y}I(x,y)(x-\\bar{x})^2$$\n",
    "$$\\mu_{02} = \\sum_{x}\\sum_{y}I(x,y)(y-\\bar{y})^2$$\n",
    "\n",
    "These central moments give an indication of how \"spread out\" the leaf is within the image in the x direction, or the y direction. We can get a *rough* idea of the length scale of the leaf region in the x and y directions by performing the following calculation\n",
    "\n",
    "$$ L_x = \\sqrt{\\frac{\\mu_{20}}{\\mu_{00}}} $$ \n",
    "$$ L_y = \\sqrt{\\frac{\\mu_{02}}{\\mu_{00}}} $$ \n",
    "$$ \\theta = \\frac{1}{2}\\arctan\\left(\\frac{2\\mu_{11}}{\\mu_{20}-\\mu_{02}}\\right) $$\n",
    "\n",
    "The last angle $\\theta$ gives us a direction which is more or less parallel with the axis of biggest variation of the leaf (the direction in which it points). We visualize these using the code block below. This code draws drawing a *CYAN* line with total length $4L_x$ through the centroid, and a *MAGENTA* line with total length $4L_y$ throw the centroid in the y direction. We also rotate these two axes by the angle $\\theta$, and then draw an ellipse around the pair of axes just defined. The angle $\\theta$ is the angle between the positive x-axis (green line) and the cyan line. Note that, while the length scale aren't exact, they are a pretty good approximation of the leaf dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_central(n=(0,len(dataset_images)-1)):\n",
    "    plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "    moments = cv2.moments(dataset_images[n].astype(np.uint8))\n",
    "    \n",
    "    area_fraction = moments[\"m00\"] / (dataset_images[n].shape[0] * dataset_images[n].shape[1])\n",
    "    plt.title(\"Leaf area %d%% of image\" % (area_fraction*100))\n",
    "    x_bar = moments[\"m10\"] / moments[\"m00\"]\n",
    "    y_bar = moments[\"m01\"] / moments[\"m00\"]\n",
    "    L_x = np.sqrt(moments[\"mu20\"] / moments[\"m00\"])\n",
    "    L_y = np.sqrt(moments[\"mu02\"] / moments[\"m00\"])\n",
    "    \n",
    "    high_x = x_bar + 2*L_x\n",
    "    low_x = x_bar - 2*L_x\n",
    "    high_y = y_bar + 2*L_y\n",
    "    low_y = y_bar - 2*L_y\n",
    "    \n",
    "    theta_rad = 0.5 * np.arctan((2 * moments[\"mu11\"])/(moments[\"mu20\"] - moments[\"mu02\"]))\n",
    "    theta = theta_rad / np.pi * 180\n",
    "    \n",
    "    #rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "    #rotated_axis1 = np.dot(rotation_matrix, np.array([high_x, 0]).T\n",
    "    #rotated_axis2 = np.dot(rotation_matrix, np.array([0, high_y]).T\n",
    "\n",
    "    #print(high_x)\n",
    "    \n",
    "    new_image = np.dstack([dataset_images[n], dataset_images[n], dataset_images[n]]).astype(np.float32)\n",
    "    cv2.ellipse(new_image, (int(x_bar), int(y_bar)), (int(2*L_x), int(2*L_y)), theta, 0, 360, (1,0,0),5)\n",
    "    #[, thickness[, lineType[, shift]]]\n",
    "    #cv2.circle(new_image, (int(x_bar), int(y_bar)), int(2*L_x), (0,1,0),2)#[, thickness[, lineType[, shift]]]\n",
    "\n",
    "    cv2.line(new_image, (int(x_bar), int(y_bar)), (int(x_bar + 2*L_x), int(y_bar)), (0,1,0),5)\n",
    "    #[, thickness[, lineType[, shift]]]\n",
    "    dx1 = 2*L_x * np.cos(theta_rad)\n",
    "    dy1 = 2*L_x * np.sin(theta_rad)\n",
    "    dx2 = 2*L_y * np.sin(theta_rad)\n",
    "    dy2 = 2*L_y * -np.cos(theta_rad)\n",
    "    cv2.line(new_image, (int(x_bar - dx1),int(y_bar - dy1)), \n",
    "             (int(x_bar + dx1), int(y_bar + dy1)), (0,1,1), thickness=3)\n",
    "    cv2.line(new_image, (int(x_bar - dx2),int(y_bar - dy2)),\n",
    "             (int(x_bar + dx2), int(y_bar + dy2)), (1,0,1), thickness=3)\n",
    "    plt.imshow(new_image)\n",
    "    \n",
    "    plt.scatter([x_bar], [y_bar], marker=\"o\", c=\"r\", s=50)\n",
    "    #plt.plot([low_x, high_x], [y_bar, y_bar], \"c-\")\n",
    "    #plt.plot([x_bar, x_bar],[low_y, high_y], \"c-\")\n",
    "    plt.show()\n",
    "\n",
    "interact(interact_with_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments as features\n",
    "\n",
    "We've just had a look at a few possible moments, just to show how they capture interesting aspects of leaf shape. Based on that observation, we can postulate that other moments may also be useful as features. OpenCV's moment calculation function actually calculates a whole selection of moment based features. Look at the below cell's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_moments(moments):\n",
    "    keys = list(moments.keys())\n",
    "    keys.sort()\n",
    "    \n",
    "    n = 0\n",
    "    mu_start = True\n",
    "    nu_start = True\n",
    "    for k in keys:\n",
    "        if (len(k) == 3):\n",
    "            print(\"%s=%30f  \" % (k,moments[k]), end = \"\")\n",
    "            print(\"\")\n",
    "        if (k[:2] == \"mu\"):\n",
    "            if mu_start:\n",
    "                mu_start = False\n",
    "                n = 0\n",
    "                print()\n",
    "            print(\"%s=%30f  \" % (k, moments[k]), end = \"\")\n",
    "            print(\"\")\n",
    "        if (k[:2] == \"nu\"):\n",
    "            if nu_start:\n",
    "                nu_start = False\n",
    "                n = 0\n",
    "                print()\n",
    "                print()\n",
    "            print(\"%s=%30f  \" % (k, moments[k]), end = \"\")\n",
    "            print(\"\")\n",
    "        n = n + 1\n",
    "        if (n % 5 == 0):\n",
    "            print()\n",
    "        \n",
    "print_moments(cv2.moments(dataset_images[0].astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `m**` and `mu**` moments are $m_{kl}$ and ${\\mu_{kl}}$ respectively. `nu**` are the so-called \"normalized central moments\"\n",
    "\n",
    "$$ \\nu_{kl} = \\frac{\\mu_{kl}}{m_{00}^{(k+l)/2+1}}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we rush into using moments as features, we need to think about what happens when we change our image in some basic ways. For example, if we change the position of the leaf, are the moments affected? \n",
    "\n",
    "In the below code, we shift around a leaf contour in the x and/or y direction. The original contour is shown in blue, the translated one in green. You can control how far the contour is moved in the x and y direction using the sliders. (You might want to zoom out a little using the contol+-, if you want to zoom in again use control++).\n",
    "\n",
    "__Exercise:__\n",
    "Execute the code below and move the image using the slider.\n",
    "Write for each of the following movements down which moments are invariant (do harly change when moved)\n",
    "1. translation (slider d_x and d_y)\n",
    "2. rotation\n",
    "3. scaling\n",
    "What does this say about the usefulness of the moments as features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution:You'll see that the raw `m**` moments are strongly affected by even a little translation, while the other moments aren't (there are extremely tiny changes in some cases, but this is due to small errors in the moment computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = get_biggest_contour(dataset_images[0])\n",
    "\n",
    "def center_scale_contour(contour):\n",
    "    moments_original = cv2.moments(contour.astype(np.float32))\n",
    "    cx = moments_original[\"m10\"] / moments_original[\"m00\"]\n",
    "    cy = moments_original[\"m01\"] / moments_original[\"m00\"]\n",
    "    centered_contour = np.array([contour[:,0]-cx, contour[:,1]-cy]).T \n",
    "    centered_contour = centered_contour / np.sqrt(moments_original[\"m00\"])\n",
    "    #centered_contour = centered_contour / np.max(np.abs(centered_contour))\n",
    "    return centered_contour\n",
    "\n",
    "def transform_contour(contour, delta_x, delta_y, theta, scale):\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "    rotated_contour = np.dot(rotation_matrix, contour.T).T\n",
    "    scaled_contour = rotated_contour * scale\n",
    "    translated_contour = np.array([scaled_contour[:,0]+delta_x, scaled_contour[:,1]+delta_y]).T\n",
    "    return translated_contour\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "def vis_transformed_contour(delta_x = (-1.0, 1.0), delta_y = (-1.0, 1.0), rotation = (-180, 180), scale = (0.5, 1.5)):    \n",
    "    center_scaled_contour = center_scale_contour(contour)\n",
    "    \n",
    "    theta = rotation * np.pi / 180\n",
    "    transformed_contour = transform_contour(center_scaled_contour, delta_x, delta_y, theta, scale)\n",
    "    \n",
    "    moments = cv2.moments(transformed_contour.astype(np.float32))\n",
    "    print_moments(moments)\n",
    "    \n",
    "    plt.scatter(center_scaled_contour[:,0],center_scaled_contour[:,1],marker=\".\", s=1, c='b')\n",
    "    plt.scatter(transformed_contour[:,0],transformed_contour[:,1],marker=\".\", s=1, c='g')\n",
    "    plt.xlim([-2,2])\n",
    "    plt.ylim([-2,2])\n",
    "    \n",
    "interact(vis_transformed_contour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous experiment, you will have seen that the different moments are sensitive to some kinds of transformations, but not to others. Still, none of these moments are insensitive to all three transformations considered (translation, rotation and scaling). It is possible, however, to combine the preceding moments into a set of moments which *are* invariant with respect to these transformations. These are know as the Hu moments after the author in the original paper that described these (M. K. Hu, \"Visual Pattern Recognition by Moment Invariants\", IRE Trans. Info. Theory, vol. IT-8, pp.179–187, 1962). There are 7 of these moments, and they are defined in terms of the normalized central moments (here using $\\eta_{kj}$ instead of $\\nu_{kj}$ used earlier in the notebook (and by OpenCV itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$   h_1 = \\eta_{20} + \\eta_{02} $$\n",
    "$$   h_2 = (\\eta_{20} - \\eta_{02})^2 + 4\\eta_{11}^2 $$\n",
    "$$   h_3 = (\\eta_{30} - 3\\eta_{12})^2 + (3\\eta_{21} - \\eta_{03})^2 $$\n",
    "$$   h_4 = (\\eta_{30} + \\eta_{12})^2 + (\\eta_{21} + \\eta_{03})^2 $$\n",
    "$$   h_5 = (\\eta_{30} - 3\\eta_{12}) (\\eta_{30} + \\eta_{12})[ (\\eta_{30} + \\eta_{12})^2 - 3 (\\eta_{21} + \\eta_{03})^2] + (3 \\eta_{21} - \\eta_{03}) (\\eta_{21} + \\eta_{03})[ 3(\\eta_{30} + \\eta_{12})^2 -  (\\eta_{21} + \\eta_{03})^2] $$\n",
    "$$   h_6 =  (\\eta_{20} - \\eta_{02})[(\\eta_{30} + \\eta_{12})^2 - (\\eta_{21} + \\eta_{03})^2] + 4\\eta_{11}(\\eta_{30} + \\eta_{12})(\\eta_{21} + \\eta_{03}) $$\n",
    "$$   h_7 = (3 \\eta_{21} - \\eta_{03})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{21} + \\eta_{03})^2] - (\\eta_{30} - 3\\eta_{12})(\\eta_{21} + \\eta_{03})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{21} + \\eta_{03})^2].$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this module, it is not important that you know the formulas above and how these are derived. All you need to know is that:\n",
    "- The Hu moments are derived from other moments that capture rough aspects of the object's shape.\n",
    "- The Hu moments are invariant with respect to translation, rotation and scaling.\n",
    "- Together, there are 7 Hu moments. So, if we only use Hu moments as our features, our feature space will now be 7-dimensional.\n",
    "\n",
    "The below code block allows you to play with translation, rotation and scaling again, but this time printing the Hu moments instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "def vis_hu_moments(delta_x = (-1.0, 1.0), delta_y = (-1.0, 1.0), rotation = (-180, 180), scale = (0.5, 1.5)):    \n",
    "    center_scaled_contour = center_scale_contour(contour)\n",
    "    \n",
    "    theta = rotation * np.pi / 180\n",
    "    transformed_contour = transform_contour(center_scaled_contour, delta_x, delta_y, theta, scale)\n",
    "    \n",
    "    moments = cv2.moments(transformed_contour.astype(np.float32))\n",
    "    hu_moments = cv2.HuMoments(moments)\n",
    "    for n, h in enumerate(hu_moments):\n",
    "        print(\"h%d=%E   \" % (n+1, h), end=\"\")\n",
    "    print()\n",
    "    \n",
    "    plt.scatter(center_scaled_contour[:,0],center_scaled_contour[:,1],marker=\".\", s=1, c='b')\n",
    "    plt.scatter(transformed_contour[:,0],transformed_contour[:,1],marker=\".\", s=1, c='g')\n",
    "    plt.xlim([-2,2])\n",
    "    plt.ylim([-2,2])\n",
    "    plt.show()\n",
    "    \n",
    "interact(vis_hu_moments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to take a moment to think about what we have now. We have taken calculated moments and turned them into values that are immune from unimportant transformations. This is very helpful, since the same object will give the same Hu moments no matter how we transform the object (in these 3 ways) in the 2D plane. That means that our classifier has far less work to do, it does not have to compensate for these changes. In that way, these features are great. On the other hand, we are summarising complex shapes with 7 values, so we do need to keep in mind that we might be losing valuable information during the summarization (feature extraction) process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a classifyer using this Hu-moments\n",
    "Now, let's get started with building a classifier! Below we take each image in the dataset and extract its Hu moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interest, this function could be used to extract moments from an image directly. We will instead extract moments from contours obtained from the original dataset images.\n",
    "#def feature_extraction(image):\n",
    "#    image = image.astype(np.float)\n",
    "#    moments = cv2.moments(image, True)\n",
    "#    return cv2.HuMoments(moments)[:,0]\n",
    "\n",
    "def feature_extraction(contour):\n",
    "    moments = cv2.moments(contour)\n",
    "    return cv2.HuMoments(moments)[:,0]\n",
    "\n",
    "dataset_X = np.array([feature_extraction(x) for x in dataset_contours])\n",
    "dataset_y = np.array(dataset_labels)\n",
    "\n",
    "print(dataset_X.shape)\n",
    "print(dataset_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to pause for a moment. In the previous model, our toy problem involved using 2 features (area and redness). We used the Euclidean distance in two dimensions for nearest neighbour search\n",
    "\n",
    "$$d_7(\\mathbf{x},\\mathbf{y})= \\sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2}. $$\n",
    "\n",
    "But, in our current problem, we have 7 Hu moments as features instead. Our feature space is now 7-dimensional, which makes it difficult to visualize. How does a decision boundary work now? The way past this problem is to realize that our intuitions from a two-dimensional or three-dimensional feature space are largely still valid (although we have to keep in mind there might be differences). The standard Euclidean distance can be easily extended to 7 dimensions. Specifically, the distance between two 7-dimensional points is just given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_7(\\mathbf{x},\\mathbf{y})= \\sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2+(x_3 - y_3)^2+(x_4 - y_4)^2+(x_5 - y_5)^2+(x_6 - y_6)^2+(x_7 - y_7)^2}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can actually use another Nearest Neighbour classifier, but instead using this 7-dimensional distance. Sure, the feature space is 7 dimensional, but we can actually imagine it will behave similarly to the 2 dimensional case we saw in the last module. In general, we often use this kind of intuition when dealing with high-dimensional problems. We can't directly think about high-dimensional feature spaces, but that doesn't mean we can get a good idea about what is going on by imagining two- or three-dimensional spaces.\n",
    "\n",
    "Let's apply some of the things we learned in the last model. Firstly, we need to define which classes there are. There are 153 classes in the original database. To get started, let's select a subset of these. We first define a list of classes we wish to keep. We've selected some for you (later on, you can experiment with changing these).\n",
    "\n",
    "The first option keeps class 0, 10 and 20. The three class problem is a nice way to introduce the concepts. Later on, we'll as you to increase the number of settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave ONLY ONE of these lines uncommented to select the problem classes.\n",
    "# It is important that you then execute this cell and all cells below in order to let the necessary changes take effect.\n",
    "\n",
    "keep_classes = set([0, 10, 20])\n",
    "#keep_classes = set([0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "\n",
    "#keep_classes = set(class_dict.keys())  # This line keeps ALL of the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code block visualizes an example from each kept class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "if (len(keep_classes) < 20):\n",
    "    for n, k in enumerate(keep_classes):\n",
    "        plt.subplot(1, len(keep_classes), n+1)\n",
    "        the_index = list(dataset_y).index(k)\n",
    "        plt.imshow(dataset_images[the_index])\n",
    "        plt.title(class_dict[k])\n",
    "else:\n",
    "    print(\"Too many classes, skipping plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the training and test set. The following code does this, and stores the images, moments and labels in `train_images, test_images, unnormalized_train_X, unnormalized_test_X, train_y, test_y`. It also skips all cases that are not in leaf species listed in `keep_classes`.\n",
    "\n",
    "Remember in the toy problem, the leaf areas and the redness values were at very different length scales (areas in the tens of thousands and rednesses less than 1). We had to rescale the values so that each feature covers more or less the same range of values. Like in the two-dimensional case, we can use `sklearn.preprocessing.RobustScaler` to rescale the data for us. This ensures that distances along the different dimensions are more or less evenly balanced in how significant they are. The result is stored in `train_X` and `test_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "keepers = [(k in keep_classes) for k in dataset_y]\n",
    "\n",
    "train_contours, test_contours, train_shapes, test_shapes, unnormalized_train_X, unnormalized_test_X, train_y, test_y = sklearn.model_selection.train_test_split([i for i, k in zip(dataset_contours, keepers) if k], [i for i, k in zip(dataset_shapes, keepers) if k], dataset_X[keepers], dataset_y[keepers], test_size=0.25)\n",
    "train_images = ContourImageAccessor(train_contours, train_shapes)\n",
    "test_images = ContourImageAccessor(test_contours, test_shapes)\n",
    "\n",
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "scaler.fit(unnormalized_train_X)\n",
    "\n",
    "train_X = scaler.transform(unnormalized_train_X)\n",
    "test_X = scaler.transform(unnormalized_test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the toy problem, we could be able to draw a scatter plot for the feature space, since the feature space was two-dimensional. However, we can still draw scatter plots for a 7-dimensional problem, but we then *SELECT* two dimensions from those seven. We then ignore the other 5 dimensions, and draw the scatter plot as for the two-dimensional case. In the code block below, a scatter plot is drawn. The points are colored according to the class. You can select which two of the seven dimensions are plotted (note, if you choose the same dimension twice, you get a straight line). There are two scatter plots. The leftmost one just plots the points as they are. But, sometimes there are very distant points that make the scatter plot difficult to interpret (most of the points are squashed to one side). Therefore, the righthand plot is also given, which plots in a way that the majority of points are visible (the really distant ones are not visible). The righthand plot therefore is a bit easier to interpret (but you have to keep in mind not all of the points are visible on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_scatter(n=(1,7), m=(1,7)):\n",
    "    plt.rcParams[\"figure.figsize\"] = (21,7)\n",
    "\n",
    "    xs = train_X[:,n-1]\n",
    "    ys = train_X[:,m-1]\n",
    "    \n",
    "    class_labels = list(set(train_y))\n",
    "    class_labels.sort()\n",
    " \n",
    "    plt.subplot(1,3,1)\n",
    "    for n, k in enumerate(class_labels):\n",
    "        keepers = train_y == k\n",
    "        plt.scatter([],[], cmap=\"tab20b\")\n",
    "    plt.title(\"Legend\")\n",
    "    plt.legend([class_dict[k] for k in class_labels])\n",
    "    plt.gca().axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    for n, k in enumerate(class_labels):\n",
    "        keepers = train_y == k\n",
    "        plt.scatter(xs[keepers], ys[keepers], cmap=\"tab20b\")\n",
    "    plt.title(\"Scatter Plot\")\n",
    "    plt.xlabel(\"Hu Moment %d\" % n)\n",
    "    plt.ylabel(\"Hu Moment %d\" % m)\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    for n, k in enumerate(class_labels):\n",
    "        keepers = train_y == k\n",
    "        plt.scatter(xs[keepers],ys[keepers], cmap=\"tab20b\")\n",
    "\n",
    "    plt.title(\"Zoomed Scatter Plot\")\n",
    "    plt.xlim([max(-5,xs.min()),min(5,xs.max())])\n",
    "    plt.ylim([max(-5,ys.min()),min(5,ys.max())])\n",
    "    plt.xlabel(\"Zoomed Hu Moment %d\" % n)\n",
    "    plt.ylabel(\"Zoomed Hu Moment %d\" % m)\n",
    "    \n",
    "if (len(keep_classes) < 20):\n",
    "    interact(interact_scatter)\n",
    "else:\n",
    "    print(\"Too many classes, skipping scatter plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: For the three class problem (classes 0, 10, 20), can you find a pair of dimensions for which it looks like the 3 classes can be separated easily?\n",
    "\n",
    "__Question__: What about the case 0, 10, 20, 30, 40, 50, 60, 70, 80, 90? Are there Hu moments which seem particularly informative? Are there two dimensions which would separate the classes easily?\n",
    "\n",
    "In the following code, we define a nearest neighbour classifier, just as in the last module. But, now we are working with real data! As you can see, once you've got your features, defining the classifier can proceed in exactly the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = classifier.predict(train_X)\n",
    "test_predict = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "cf_matrix = sklearn.metrics.confusion_matrix(test_y, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(keep_classes) < 20:\n",
    "    print(cf_matrix)\n",
    "else:\n",
    "    print(\"Number of classes to large, not printing confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "plt.imshow(cf_matrix, cmap=\"gray\", vmin=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can be calculated using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.accuracy_score(test_y, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Compare the classifier performance of the three, ten and complete 153 class problems. \n",
    "* Have a look at the confusion matrix plotted as an image, what does this say about how the nearest neighbour search is performing?\n",
    "* How does the accuracy in each problem compare with random guessing? (Remember the number of classes in the problem influences this).\n",
    "* In the ten problem case specifically, look at the confusion matrix. Which classes are being confused with each other? Referring to some of the sample images from these classes plotted in an earlier cell, can you say something about why these classes are be difficult to distinguish?\n",
    "* In the command `classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`, there is a way of specifying how many nearest neighbours we check when trying to classify an image. If `n_neighbors` is\n",
    "5 instead, we would look at the closest 5 points, and select the class that gives a majority vote. Explore how the results change with changes to this number of neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier descriptors\n",
    "\n",
    "We will start our discussion by looking at Fourier features. These make use of a signal processing tool called the Fourier transform. We won't look in detail at the mathematics behind the Fourier transform, we will instead try to demonstrate why it is useful as a way of calculating features.\n",
    "\n",
    "Fourier descriptors work by encoding slow changes in a contour separately from fast changes (technically we talk about low frequency and high frequency components of the contour). We will return to this shortly, but we must first define some helper functions.\n",
    "\n",
    "The amount of features in a Fourier descriptor is dependent on the amount of points in the contour it is analysing. For machine learning purposes, we need the number of features to always be the same, otherwise we can't, for example, calculate the distance in the feature space. Unfortunately, the number of contour points we get back from OpenCV depends on how many pixels there are in the outer edge of each leaf. The bigger a leaf, the bigger the number of points in its outer contour. We need to address this situation before continuing.\n",
    "\n",
    "The following code block allows us the \"resample\" a contour so that it contains a specified number of points. It works by interpolating between neighbouring points in the contour. The mathematical details aren't important, but you should get an intuitive idea of what we mean here. In the following code block, you can select a leaf from the database. You can also control the number of points in the outer contour. You can then see what the result of resampling the leaf contour is. In particular, look at what happens when the number of points is really low, you'll see that the resampled contour is poor if the number of points is too low. Be sure to also look at different leaves.\n",
    "\n",
    "__Question__: Based on your exploration below, what seems like a good minimum number of resampling points (look in the figure title for the number of sample points, the slider you are adjusting controls the power of two of actual resampling points)? Be sure to look at different kinds of leaves. What makes a leaf simple or difficult to resample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.transform\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "\n",
    "def resample_contour(contour, num_points):\n",
    "    themin = np.min(contour)    \n",
    "    contour = contour - themin\n",
    "    \n",
    "    themax = np.max(contour)\n",
    "    contour = contour / themax\n",
    "    \n",
    "    x = skimage.transform.resize(contour[:,0:1].astype(np.float32), (num_points, 1), order=3, mode=\"wrap\")\n",
    "    y = skimage.transform.resize(contour[:,1:2].astype(np.float32), (num_points, 1), order=3, mode=\"wrap\")\n",
    "    \n",
    "    new_contour = np.hstack([x,y])\n",
    "    new_contour = new_contour * themax + themin\n",
    "    \n",
    "    #start_x = (new_contour[0,0] + new_contour[-1,0]) * 0.5\n",
    "    #start_y = (new_contour[0,1] + new_contour[-1,1]) * 0.5\n",
    "    #new_contour[0,0] = start_x\n",
    "    #new_contour[-1,0] = start_x\n",
    "    #new_contour[0,1] = start_y\n",
    "    #new_contour[-1,1] = start_y\n",
    "    \n",
    "    return new_contour\n",
    "    \n",
    "def interact_resample(image_num=(0, len(dataset_images)-1), point_power=(1,16)):  \n",
    "    num_points = 2 ** point_power\n",
    "    contour = get_biggest_contour(dataset_images[image_num])\n",
    "    contour = center_scale_contour(contour)\n",
    "    resampled = resample_contour(contour, num_points)\n",
    "    plt.plot(contour[:,0], contour[:,1])\n",
    "    plt.plot(resampled[:,0], resampled[:,1])\n",
    "    plt.title(\"%d Point Resampling\" % num_points)\n",
    "    \n",
    "interact(interact_resample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by selecting 256 as the number of resampling points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_points = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first give you an idea of what information the Fourier descriptor captures. First, let's define a function which returns the Fourier descriptor. We do this in the next code block. There is also code in this block for visualizing a contour from the database, and plotting the Fourier descriptor from this database image. The Fourier descriptor won't look particularly useful at this point. We will clarify why the Fourier descriptor is a nice representation in a later code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, min_log=-20):\n",
    "    the_min = np.exp(min_log)\n",
    "    safe_x = np.clip(x, the_min, np.inf)\n",
    "    return np.log(safe_x)\n",
    "    \n",
    "def get_fourier_descriptor(contour, return_freqs=False):\n",
    "    #contour = center_scale_contour(contour)\n",
    "    f = contour[:,0] + contour[:,1] * (0+1j)\n",
    "    fft = np.fft.fft(f)#[1:]\\\n",
    "    fft_range = np.arange(fft.shape[-1])   \n",
    "    freq_max = fft_range[-1]\n",
    "    \n",
    "    fft = np.fft.fftshift(fft)\n",
    "    fft_range = np.fft.fftshift(fft_range)\n",
    "    \n",
    "    middle = fft_range[-1]\n",
    "    uppers = fft_range>middle\n",
    "    fft_range[uppers] = fft_range[uppers] - freq_max - 1\n",
    "    \n",
    "    non_zero_freqs = fft_range != 0\n",
    "\n",
    "    fft = fft[non_zero_freqs]\n",
    "    fft_range = fft_range[non_zero_freqs]\n",
    "    \n",
    "    feats = np.absolute(fft)\n",
    "    feats = feats / np.sum(feats)\n",
    "    \n",
    "    if (return_freqs):\n",
    "        return feats, fft_range\n",
    "    else:\n",
    "        return feats\n",
    "\n",
    "def plot_descriptor(freqs, descriptor, min_log=-20):\n",
    "    plt.plot(freqs,safe_log(descriptor, min_log=min_log))\n",
    "    plt.ylim([min_log,0])\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Fourier Coefficient Magnitude\")\n",
    "    \n",
    "\n",
    "def interact_fourier(n=(0,len(dataset_images)-1)):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "    image = dataset_images[n]\n",
    "    contour = get_biggest_contour(image)    \n",
    "    resampled_cont = resample_contour(contour, resample_points)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(contour[:,0], contour[:,1])\n",
    "    plt.title(\"Database Image\")\n",
    "    plt.subplot(1,2,2)\n",
    "    min_log = -20\n",
    "    #plt.plot(safe_log(get_fourier_descriptor(resampled_cont), min_log=min_log))\n",
    "    descriptor, freqs = get_fourier_descriptor(resampled_cont, True)\n",
    "    plot_descriptor(freqs, descriptor, min_log=min_log)\n",
    "    plt.title(\"Fourier Descriptor\")\n",
    "interact(interact_fourier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's develop an intuition for what the Fourier descriptor contains. Below we visualize the Fourier descriptor in a similar way as above. However, we allow you to drop the highest (negative or positive) frequency components of the descriptor. We then plot the curve associated with this more limited frequency range. By doing this, you can get an idea of what the high frequency components encode, and what the lower frequency components encode.\n",
    "\n",
    "__Question__: What do you observe as you decrease or increase the frequencies kept in the descriptor? Based on your exploration, describe what the low and high frequency components are associated with.\n",
    "\n",
    "Solution: The low frequency components are assocated with the broad approximate shape of the leaf. The high frequency components are associated with more detailed features of the leaves, such as small zigzaw patterns in the outer edges.\n",
    "\n",
    "Technical note: The Fourier descriptor does not actually encode all the information necessary to reconstruct the leaf contour. This is because we throw away the component of the Fourier transform that is dependent on the leaf's rotation. But, the Fourier descriptor does maintain the same separation of frequency components as the original Fourier Transform, so we can still make general statements about which frequencies have which role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_freqs(contour, dropped_center_freqs=0):\n",
    "    #contour = center_scale_contour(contour)\n",
    "    f = contour[:,0] + contour[:,1] * (0+1j)\n",
    "    orig_fft = np.fft.fft(f)\n",
    "    freqs = np.arange(orig_fft.shape[0])\n",
    "    mid_freq = orig_fft.shape[0] // 2\n",
    "    kept_freqs = np.abs(freqs - mid_freq) >= dropped_center_freqs\n",
    "    \n",
    "    zeroed_fft = orig_fft * kept_freqs\n",
    "    zeroed_ifft = np.fft.ifft(zeroed_fft)\n",
    "    \n",
    "    xs = np.real(zeroed_ifft)\n",
    "    ys = np.imag(zeroed_ifft)\n",
    "    return np.array([xs, ys]).T\n",
    "\n",
    "def interact_dropped_freqs(n=(0,len(dataset_images)-1), dropped_points=(0, resample_points // 2 - 1)):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "    image = dataset_images[n]\n",
    "    contour = get_biggest_contour(image)    \n",
    "    resampled_cont = resample_contour(contour, resample_points)\n",
    "    dropped_cont = drop_high_freqs(resampled_cont, dropped_points)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(contour[:,0], contour[:,1])\n",
    "    plt.plot(dropped_cont[:,0], dropped_cont[:,1])\n",
    "    plt.subplot(1,2,2)\n",
    "    dropped, freqs = get_fourier_descriptor(dropped_cont, True)\n",
    "    plot_descriptor(freqs, get_fourier_descriptor(resampled_cont))\n",
    "    plot_descriptor(freqs, dropped)\n",
    "interact(interact_dropped_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea around what kind of information is encoded by the Fourier descriptor, there is one more thing we need to check. What is the effect of translation, rotation and scaling on the Fourier descriptor. Below is a piece of code that allows you to select a database image, and the move, rotate or scale the image. The Fourier descriptor of the transformed curve is plotted on the right-hand side. Try it out and see what the effect is of moving a leaf around in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = get_biggest_contour(dataset_images[0])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (27,10)\n",
    "def vis_transformed_contour_fourier(database_image=(0,len(dataset_images)-1), delta_x = (-1.0, 1.0), delta_y = (-1.0, 1.0), rotation = (-180, 180), scale = (0.5, 1.5)):    \n",
    "    #image = dataset_images[database_image]\n",
    "    #contour = get_biggest_contour(image)     \n",
    "    contour = dataset_contours[database_image]\n",
    "    resampled_contour = resample_contour(contour, resample_points)\n",
    "    center_scaled_contour = center_scale_contour(resampled_contour)\n",
    "    \n",
    "    theta = rotation * np.pi / 180\n",
    "    transformed_contour = transform_contour(center_scaled_contour, delta_x, delta_y, theta, scale)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(center_scaled_contour[:,0],center_scaled_contour[:,1],marker=\".\", s=1, c='b')\n",
    "    plt.scatter(transformed_contour[:,0],transformed_contour[:,1],marker=\".\", s=1, c='g')\n",
    "    plt.xlim([-2,2])\n",
    "    plt.ylim([-2,2])\n",
    "    plt.subplot(1,2,2)\n",
    "    desc, freqs = get_fourier_descriptor(transformed_contour, True)\n",
    "    plot_descriptor(freqs, desc)\n",
    "    plt.title(\"Fourier Descriptor of Transformed Curve\")\n",
    "  \n",
    "interact(vis_transformed_contour_fourier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around a while, it should be clear that the Fourier descriptor is immune to translation, rotation and scaling. This is great, because whatever classifier we use does not need to take into account that a given leaf might move around. It seems surprising that a descriptor that encodes so much of the original shape is unaffected by such changes in its appearance. \n",
    "\n",
    "We won't go into the mathematics of the Fourier transform, but you should at least get a feeling that Fourier descriptors encode a lot more about the shape than the 7 Hu moments can. Fourier descriptors are also more flexible. For example, we can choose how many frequencies we will keep in the final descriptor. The number of points in the resampled contour is another important aspect we can control directly.\n",
    "\n",
    "Finally, we're ready to perform some machine learning on the Fourier descriptors. The following cells perform an analysis very similar to that of the toy problem, and the Hu moment version of the leaf problem. It's important to note again that, even though the features are very different, the same code can be used to train and evaluate the classifier. \n",
    "\n",
    "First we define how features are extracted, and then extract the Fourier descriptors. Please note that we can here control the number of resampling points manually (so that you can more easily experiment with these)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning_resample_points = 48\n",
    "\n",
    "#def feature_extraction(image):\n",
    "#    contour = get_biggest_contour(image)\n",
    "#    return get_fourier_descriptor(resample_contour(contour, machine_learning_resample_points))\n",
    "def feature_extraction(contour):\n",
    "    return get_fourier_descriptor(resample_contour(contour, machine_learning_resample_points))\n",
    "\n",
    "dataset_X = np.array([feature_extraction(x) for x in dataset_contours])\n",
    "dataset_y = np.array(dataset_labels)\n",
    "\n",
    "print(dataset_X.shape)\n",
    "print(dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "keep_classes = set([0, 10, 20])\n",
    "#keep_classes = set([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110])\n",
    "#keep_classes = set(class_dict.keys())\n",
    "\n",
    "keepers = [(k in keep_classes) for k in dataset_y]\n",
    "\n",
    "train_contours, test_contours, train_shapes, test_shapes, train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split([i for i, k in zip(dataset_contours, keepers) if k], [i for i, k in zip(dataset_shapes, keepers) if k], dataset_X[keepers], dataset_y[keepers], test_size=0.25)\n",
    "train_images = ContourImageAccessor(train_contours, train_shapes)\n",
    "test_images = ContourImageAccessor(test_contours, test_shapes)\n",
    "\n",
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "scaler.fit(train_X)\n",
    "\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(keep_classes) < 20):\n",
    "    for n, k in enumerate(keep_classes):\n",
    "        plt.subplot(1, len(keep_classes), n+1)\n",
    "        the_index = list(dataset_y).index(k)\n",
    "        plt.imshow(dataset_images[the_index])\n",
    "        plt.title(class_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import sklearn.ensemble\n",
    "\n",
    "# You can choose the kind of classifier by leaving ONLY ONE of the following lines uncommented.\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "#classifier = sklearn.ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = classifier.predict(train_X)\n",
    "test_predict = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "cf_matrix = sklearn.metrics.confusion_matrix(test_y, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cf_matrix(cf_matrix):\n",
    "    spaces = int(np.log10(cf_matrix.max())) + 2\n",
    "    format_string = \"%%%dd\" % spaces\n",
    "    for n in range(cf_matrix.shape[0]):\n",
    "        for m in range(cf_matrix.shape[1]):\n",
    "            print(format_string % cf_matrix[n][m], end=\"\")\n",
    "        print()\n",
    "    \n",
    "if (len(keep_classes) < 20):\n",
    "    print(' The confusion matrix:' )\n",
    "    print_cf_matrix(cf_matrix)\n",
    "else:\n",
    "    print(\"Number of classes too high, skipping printing confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "plt.imshow(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.accuracy_score(test_y, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Compare the classifier performance of the three, ten and complete 153 class problems. And answer the following questions:\n",
    "* Have a look at the confusion matrix plotted as an image, what does this say about how the nearest neighbour search is performing?\n",
    "* How does the accuracy in each problem compare with random guessing? (Remember the number of classes in the problem influences this).\n",
    "* In the ten problem case specifically, look at the confusion matrix. Which classes are being confused with each other? Referring to some of the sample images from these classes plotted in an earlier cell, can you say something about why these classes are be difficult to distinguish?\n",
    "* What is the influence of the number of resampling points on the accuracy? A great number of resampling points means that the original leaf shape is better preserved, but how does this influence the actual machine learning results? Motivate your statements with experiments performed with different values for the number of resampling points.\n",
    "* In the command `classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`, there is a way of specifying how many nearest neighbours we check when trying to classify an image. If `n_neighbors` is\n",
    "5 instead, we would look at the closest 5 points, and select the class that gives a majority vote. Explore how the results change with changes to this number of neighbours.\n",
    "* Scikit learn implements a great variety of classifiers. One well-known classifier is called a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "Try it out by uncommenting the following line where the classifier is created \n",
    "`classifier = sklearn.ensemble.RandomForestClassifier(n_estimators=100)`\n",
    "Remember to comment the other classifier line which creates the Nearest Neighbour classifier. Comment on the difference in accuracy between the classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
